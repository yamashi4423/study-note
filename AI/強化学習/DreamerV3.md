# Dreamer V3

### ざっくり

DreamerV2からの改良点

- ロバストネス技術
  - 観測スケールの調整
    - 入力データを対数スケールに圧縮。大きな値や負の値を効果的に扱う。
  - KLバランスとフリービット
    - KL損失が過剰に増加するのを防ぐ。学習の安定性を保つためフリービットを使用。
  - Symexp Twohot回帰
    - 報酬や価値の予測において、カテゴリ分布の範囲を指数的に拡張。スケールの違いに適応。
  - Unimix法
    - エンコーダやダイナミクス予測器の確率分布に1%の一様分布を加える。確率が極端な値にならないように。
- ネットワークアーキテクチャ
  - ブロック構造をもつGRU
  - RMSNorm正規化・SiLU活性化関数
- 最適化アルゴリズム
  - Adaptive Gradient Clipping
    - 勾配のスケールを自動で調整。損失関数や学習率に依存しない安定性
  - LaPropオプティマイザ
    - RMSPropにモーメンタムを加えた新しい最適化手法
- リプレイバッファ
  - 潜在状態をリプレイバッファに格納
  - オンラインキューを使用。学習とデータ収集を並行実行。
- モデルサイズのスケーラビリティ
  - 12Mから400Mまでスケール可能に


### 世界モデル

- 損失関数
  - 予測損失
    $$
    L_{pred}(\phi) = - \ln p_{\phi}(x_t | z_t, h_t) - \ln p_{\phi}(r_t | z_t, h_t) - \ln p_{\phi}(c_t | z_t, h_t)
    $$
    
    - 画像、報酬、エピソード継続フラグの再構成誤差
    
  - 動力学損失
    $$
    L_{dyn} (\phi) = \max (1, KL[sg(q_{\phi}(z_t|h_t, x_t)) || p_{\phi}(z_t|h_t)])
    $$
    
    - 事前分布を学習するKLダイバージェンス
    - KLバランス + フリービット
    
  - 表現損失
    $$
    L_{rep} (\phi) = \max (1, KL[q_{\phi}(z_t|h_t, x_t) || sg(p_{\phi}(z_t|h_t))])
    $$
    
    - 事後分布を学習するKLダイバージェンス
    - DreamerV2までは、たぶんposteriorを教師にしてKLバランスしてたから、たぶんこの項はなかった
  
- フリービット
  - 正則化項を緩和する手法
    - 強い正則化が加わると、入力データに十分な情報を保持できなくなる
    - 特定の次元でのKL損失が過剰に大きくなり、学習が不安定になる
    - 感覚: VAEよりオートエンコーダーの方がきれいじゃね？ってこと
  - 解決策
    - 各次元のKL損失に許容範囲（閾値を設定）。その範囲内では損失は無視
  - DreamerV3における役割
    - 学習の安定
      - KL損失の急激な増加することがあるのを防ぐ
    - 複雑なタスクでは、潜在表現に視覚的な詳細な情報が必要

### Actor-Critic

- DreamerV2とほぼ同じ。Criticだけ、エピソードの継続フラグが追加しただけ？

### プログラム

DreamerV3のpytorch実装について、なんとなくまとめ

#### WorldModel

- 学習
  - 観測データをエンコード
  - $deter(t), stoch(t), action(t)$から、$deter(t+1), prior(t+1), posterior(t+1)$ を予測（$posterior(t+1)$は、$embed$も必要）
    - あくまで$prior(t+1)$は、$posterior(t+1)$の推論（つまり多変量正規分布）を学習するために必要
  - KLダイバージェンスを計算

### RSSM

- observeメソッド

  - 時系列データ$embed, action$から、$prior, post$ を生成

    ```python
    post, prior = tools.static_scan(
        lambda prev_state, prev_act, embed, is_first: self.obs_step(
            prev_state[0], prev_act, embed, is_first
        ),
        (action, embed, is_first),
        (state, state),
    )
    ```

    - ん！？これって毎回$t$まで生成してんの！？
      - たとえば、$prior(t=100),post(t=100)$を求めるために、$t=0$から$t=99$までの$prior, post$を生成してんの？効率悪くね？
        - GPTの回答: バッチ処理、ステップのスキップ、**キャッシュの利用**をすべきとのこと。やっぱり...。キャッシュしちゃうとまずかったりする？
        - => そもそも、$t$の長さ（最大値）は、バッチサイズである。そのため、バッチサイズ以前の時系列は考慮することはできない

- obs_stepメソッド

  - 観測データ（embed）を用いて潜在状態（post:後分布）を更新し、潜在状態の事前予測（prior:事前分布）も計算するメソッド。つまり、$embed, s_{t}$から多変量正規分布で$prior_{t+1}, post_{t+1}$を生成してるってこと。

#### Dreamerクラス

- call
  - 観測 obs を受け取り、エージェントの内部状態 state に基づいて、次の行動と新たな内部状態を返す

#### simulate関数

- 実世界環境とエージェントとのやりとりをする関数。ふつうの強化学習の学習の部分みたいなかんじ（WorldModelでは学習はしない）。
- アルゴリズム
  - Doneかどうかのチェック
    - Doneなら環境のリセット

  - 観測から行動を選択
    - LLMエージェント（親）の場合とDreamerエージェント（子）の場合でそれぞれ行動
    - すべての環境に対する行動の配列$\_actions$として保存

  - 選択した行動から観測を選択
  - ステップ数の更新
  - 各環境の遷移情報（観測、アクション、報酬、割引率）をキャッシュに保存
  - エピソードが終了した環境のログを保存


### 知っておきたいこと

- 改良しそうなところ
  - 世界モデルの1ステップ（obs_stepメソッド）
  - 世界モデルのKLダイバージェンス計算
  - 
- カテゴリカル分布の微分（ストレートスルー勾配とかいうの）が、どうなってるのか見ておきたい

### プログラムのわかんねポイント

DreamerV3のpytorch実装について

#### WorldModel

- dyn_scale, rep_scaleとは？
- エントロピーの計算はなぜここでやる？先にやっておくって感じ？
- 最後になぜpostをdetachする？もちろん、$L_{rep}(\phi)$の計算では固定するけど、ここでやんの？なんとなくself.dynamics.kl_loss()とかでやるんじゃないんだ

### おもったこと

- RSSM以外の逐次VAEってどんなのがあるんかなー
  - 