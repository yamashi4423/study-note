# Dreamer V2

### ざっくり

Dreamerから以下を変更

- RSSM

  - 潜在変数をカテゴリ変数にしたよ

    - 目的: 表現の柔軟性、マルチモーダルデータを効率的に、スパースな表現による汎化。
    - ストレートスルー勾配を用いて微分。

  - 割引因子 $\gamma_t$ も学習するよ

    - 画像予測: 単位分散を持つ対角ガウス分布の平均
      - 画像予測がラク！
        - 各ピクセルの対数尤度の和が、画像全体の対数尤度になるから。
    - 報酬予測: 単位分散を持つガウス分布
    - 割引予測: ベルヌーイ分布

  - 決定的推論はGRU

  - ELBOのKLダイバージェンス最小化の際、posterior分布とprior分布の学習速度を調整するよ

    - posterior: 現在の観測から予測

    - prior: 観測なしで予測

    - 明らかにposteriorの方が速く学習するので、posteriorが過剰に学習しないように制約を加える

    - KLダイバージェンスをこのようにする: それぞれの学習を指数加重平均
      $$
      KL_{balanced} = \alpha KL(p||stop_grad(q)) + (1 - \alpha) KL(stop_grad(p)||q)
      $$

      - prior学習: $\alpha$を高く設定し、迅速に学習されるようにする
      - posterior学習: $\alpha$ を低く設定し、あまり学習しないようにする

### Actor-Critic

- Actor
  - エンコード状態$s_t$のシーケンスを生成し、アクションを選択
  - カテゴリ分布。ストレートスルー勾配で学習？
  - 損失: REINFORCE項 + Dynamics Backprop項 + Entropy Regularizer項
    - REINFORCE項: $\ln p_\psi (\hat{a_t} |\hat{z_t}) sg(V_t^\lambda - v_\xi (\hat{z_t}))$ いつものActor-Critic。偏りはないが、高分散。
    - Dynamics Backprop項: $V_t^\lambda$ 偏りはあるが、低分散のストレートスルー勾配。収益の期待値そのままなので、シンプルな方策勾配法みたいなかんじ。
      - こっちの方が速く学習することができる
    - REINFORCE項、Dynamics Backprop項は、ハイパーパラメータ$\rho$ で指数加重平均。
      - $\rho$が大: Atariなど。高分散。
      - $\rho$が小:  連続制御など。偏り大。
    - Entropy Regularizer項: 
      - $\mu$が大: Atariなど。
      - $\mu$が小:  連続制御など。
- Critic
  - Actorが行動した結果得られる将来の累積報酬を予測
  - たぶんDreamerと同じ。$\lambda$リターンの指数加重平均

### アブレーション実験結果

- ディスクリート潜在変数
  - ガウス潜在変数よりも良かった
  - 理由: 非連続的な変化や、マルチモーダルな分布をモデル化する能力が高い
- KLバランス
  - 良くなった
- 画像の学習 VS 報酬の学習
  - 画像情報からの勾配は学習に不可欠だが、報酬の勾配は不要な場合が多い
- Actor-Critic
  - REINFORCE項が効果的
- GRUのLayer Normalization
  - タスクによる

### わからないところ・不安なところ

- ELBOのKLダイバージェンスが、なぜpriorとposteriorに対応しているのか

  - VAEのときはエンコーダーを標準正規分布に近づけることに対応していたが

- 対角ガウス分布って何？

  - 多次元正規分布の特殊なケース。共分散行列が対角行列。
  - 各次元間の相関がないよってこと。
    - **え...。VAEのエンコーダーされた次元に相関あるだろ...。**
  - 対角ガウス分布をは、次のようにそれぞれの次元の正規分布の積で表せる

  $$
  p(x) = \Pi_{i=1}^{n} N (x_i | \mu_i, \sigma_i^2)
  $$

- なぜ対角ガウス分布？というか、どう実装する？

  - VAEがそもそも対角ガウス分布らしい？

- Actor-Criticなのこれは？

  - Actor-Criticは、状態価値関数が最大化するような方策を学習すること
    - 方策勾配法は方策を学習すること。それに状態価値関数を使ってるよって感じ？価値反復法は方策は学習しない。状態価値関数が最大である方策をグリーディーに選択しているだけ。

- Dynamics Backpropだけストレートスルー勾配ですみたいなこと書いてあるけど、どっちもストレートスルー勾配なのでは？だって行動はディスクリートなんでしょ？

  - プログラム追うか...

- あれ、「対数尤度の最大化 = 再構成誤差の最小化」になるのってなんでだっけ？

  - 対数尤度の最大化の近似がELBOだよね。てことは再構成誤差だけでなくて正則化項も含まれてない？
    - **ん？というか、対数尤度求められんの？？？**



### 改良できそうなところ

#### RSSMモデル部分

- 階層型VAEを用いる
  - たぶん学習量多い？ので、時系列方向の計算を並列化（Transformer的な）したい。
  - 細かいところをみたいときと、潜在的なところを見たいときがあると思う。それを自動で選択したい。ある意味、確率的潜在変数ってのはそういうことなのでは？
- タスクの階層化

#### 幼児モデル的な改良

- 意図の予測（親エージェントからの学習）



### メモ

- 結局やりたいのは、世界モデルの学習。ここが、LLMの学習みたいな感じになっているとみる。